---
title: "Kristen Boysen (username:knboysen) Site"
author: "Kristen Boysen"
date: "January 15, 2016"
output:   
  html_document:
    toc: true
    toc_depth: 2
---

```{r echo=F}
setwd
# set working directory if has child directory
dir_child = 'students' 
if (dir_child %in% list.files()){
  if (interactive()) {  
    # R Console
    setwd(dir_child)
  } else {              
    # knitting
    knitr::opts_knit$set(root.dir=dir_child)  
  }
}
```

## Content
        
I am interested on climate impacts on a large landscape scale, how ecosystems and species will adapt, and how we as managers can help improve the resilience of these environments. 
        
## Techniques

There are several techniques I am looking forward to learning:

1. Dealing with large datasets
2. Using Git to collaborate with other researchers
3. Visualizing and sharing data analytics
        
## Data
        
I did make up some data today during class:
  
```{r echo= FALSE}
impacts<- c(1,1,3, 3,4,5,6, 6, 6, 7)
temp<- c(20,22, 24, 25.5,28, 28, 32, 31, 29, 32)

sea_data<-data.frame(impacts, temp)
colnames(sea_data)<- c("sea_rise", "ave_temp")

model<-lm(sea_rise~ ave_temp, data=sea_data)
plot(sea_data$ave_temp, sea_data$sea_rise, ylab = "Sea Level Rise (m)", xlab="Temp (C)", main="Invented Sea Level Rise with Rising Temperatures")
abline(model, col="red")
```

I do think United National Environmental Programme will have some interesting data, as will NOAA.  
 
## Penguins
Here is a photo of a molting, juvenile Gentoo penguin (*Pygoscelis papua*) on [King George Island](https://www.google.com/maps/place/King+George+Island,+Antarctica/@-62.0757944,-58.8658154,9z/data=!3m1!4b1!4m2!3m1!1s0xbc738f0fdffd7975:0x759c74bafe566d71), Antarctica. I took this photo in February, 2011 while working for NOAA. 

<img src="images/knboysen_gentoo.jpg" width="400px"/>

##Sea Level Rise Data from UNEP

I downloaded data from the United National Environmental Programme's [Environmental Data Explorer](http://ede.grid.unep.ch/) that contains the percent of total land area that is below 5m elevation for each country. Below are summary statistics of that data:

```{r}
# read csv
sea_level_unep_kb <- read.csv("data/knboysen_sea_level_unep.csv")
      
# output histogram
hist(sea_level_unep_kb$Percent_land_under_5m, xlab="Percent Land Below 5m", main="Histogram of UNEP Elevation Data", col ="blue")
```

This data set includes `r nrow(sea_level_unep_kb)` countries. Below is a table of the `r nrow(subset)` countries that are completely (100% of land area) below 5m of elevation
```{r}
subset<- subset(sea_level_unep_kb, Percent_land_under_5m==100)
print(subset[,c(1,3,15)])
```

The mean percent of land under 5m is `r mean(na.omit(sea_level_unep_kb$Percent_land_under_5m))`%. 

##Data Wrangling

Install Packages
```{r, eval=F}
# Run this chunk only once in your Console
# Do not evaluate when knitting Rmarkdown

# list of packages
pkgs = c(
  'readr',        # read csv
  'readxl',       # read xls
  'dplyr',        # data frame manipulation
  'tidyr',        # data tidying
  'nycflights13', # test dataset of NYC flights for 2013
  'gapminder')    # test dataset of life expectancy and popultion

# install packages if not found
for (p in pkgs){
  if (!require(p, character.only=T)){
    install.packages(p)
  }
}
```


##utils::read.csv
Traditionally, you would read a CSV like so:
```{r}
d = read.csv('../data/r-ecology/species.csv')
d
head(d) ##species_ID is a factor
summary(d)
```

readr::read_csv
Better yet, try read_csv:
```{r}
library(readr)

d = read_csv('../data/r-ecology/species.csv')
d
head(d) ##now the specices_ID is a "character". why does this matter, BB?!
summary(d)
```

dplry::tbl_df
Now convert to a dplyr table:
```{r}
library(readr)
library(dplyr)

d = read_csv('../data/r-ecology/species.csv') %>%
  tbl_df()
d
head(d)
summary(d)
glimpse(d)
```

```{r}
b=read_csv('C:/Users/Kristen/Documents/BREN/winter16/env_info/env-info/data/r-ecology/surveys.csv') %>%
  select(species_id, year) %>%
  filter(species_id== "AB") %>%
  group_by(species_id, year) %>%
  summarize(count = n())

```

###elegance with dplry 
week 3 individual assignment

```{r}
library(readr)
library(dplyr)
#read is csv
surveys = read_csv('../data/r-ecology/surveys.csv')

surveys %T>% ##tee operator is good for printing or plotting that wouldn't ouput a return usually
  glimpse() %>%
  select(species_id, year) %>% #selected columns
  filter(species_id == 'NL') %>% ##with specific row entries
  group_by(year) %>%
  summarize(n=n()) %T>%  ##summarize n counts
  glimpse() %>% ##view the table before writing it. it seems to work. 31 "NL" counts in 1977
  write_csv('data/surveys_kboysen.csv') ##hooray it works!
  
  
```

###Wrangling Webinar
####Piping

```{r, echo= F}
library(devtools)
devtools::install_github("rstudio/EDAWR")
library(EDAWR)
```

```{r}
library(tidyr)
library(dplyr)

select(tb, child:elderly) ##verses
tb %>% select(child:elderly)

#yay piping
```


###tidyr
```{r}
library(tidyr)
##cases, a dataset about TB

gather(cases, "year", "n", 2:4) ##cases is the dataframe we are reshaping, "year"/"n" are the new column names, and we are collapsing columns 2-4

##the opposite function is "spread"
#use "pollution" dataset

spread(pollution, size, amount)
#creates a new columns for each size (large & small), and populates it with the "amount"


##separate!
storms2<- separate(storms, date, c("year", "month", "day"), sep = "-") ##separates the date of the storm by the year month day (dash separates the original date) this is very cool

##unite!
unite(storms2, "date", year, month, day, sep = "-") ## put it back together again


```

###Dplyr

```{r}
library(nycflights13)

###dplyr ways to access info
##select() extracts variables
##filter() extracts exisiting observations
#mutate()  derive new variables
##summarise() ##change the unit of analysis

## - selects everything but
## : selects range
##contains, ends_with, everthing(), matches(), num_range(), one_of(), starts_with()

filter(storms, wind>=50, 
       storm %in% c("Alberto", "Alex", "Allison"))

#mutate

mutate(storms, ratio =pressure/wind)
mutate(storms, ratio =pressure/wind, inverse= ratio^-1)
#useful mutate functions

mutate(storms, cummean(pressure)) ##creates new column with cummulative mean
mutate(storms, percent_rank(wind)) ##ranks them 0-1
mutate(storms, dense_rank(wind)) ##ranks them 1(low)-6
mutate(storms, min_rank(wind))

##window functions
## i don't get these


##summarise

pollution %>% summarise(median=median(amount), variance=var(amount))

pollution %>% summarise(mean= mean(amount), sum= sum(amount), n=n())

##arrange() sorts data frame

##desc makes it descending. default is ascending

##piping again

storms %>% filter(wind >=50) 
#vs
filter(storms, wind>=50)

storms %>%
  mutate(ratio = pressure/wind) %>%
  select(storm, ratio)

#ctrl shift m!

# %>% 

```
##Unit of analysis

```{r}
#group by 
pollution %>%  group_by(city)
pollution %>%  group_by(city) %>% summarise(mean= mean(amount), sum= sum(amount), n=n())

pollution %>%  group_by(size) %>% summarise(mean=mean(amount))

pollution %>%  ungroup()

##tb example

```
##joining data

```{r}
y<- data.frame(x1=c("a","b","c"), x2=(c(1,2,3)))
y[] <- lapply(y, as.character)
z<-data.frame(x1=c("b","c","d"),x2=(c(2,3,4)))

z[] <- lapply(z, as.character)

##is this the best way to change factors to characters?
##are we losing info by changes the #s (x2) into characters?
##bind_cols
bind_cols(y,z)
##bind_rows
bind_rows(y,z)
##Union
union(y,z)
##intersect
intersect(y,z)
##setdiff
setdiff(y,z)

##d doesn't show up here? 

left_join(songs, artists, by="name")

inner_join(songs, artists, by="name")
```

##Week 04-- Tidyr
### EDAWR

```{r EDAWR, eval=F}
# install.packages("devtools")
# devtools::install_github("rstudio/EDAWR")
library(EDAWR)
help(package='EDAWR')
?storms    # wind speed data for 6 hurricanes
?cases     # subset of WHO tuberculosis
?pollution # pollution data from WHO Ambient Air Pollution, 2014
?tb        # tuberculosis data
#Viewd(storms)
#View(cases)
#View(pollution)
```

### slicing

```{r traditional R slicing, eval=F}
# storms
storms$storm
storms$wind
storms$pressure
storms$date

# cases
cases$country
names(cases)[-1]
unlist(cases[1:3, 2:4])

# pollution
pollution$city[c(1,3,5)]
pollution$amount[c(1,3,5)]
pollution$amount[c(2,4,6)]
##single equals sign sets the value, double equals sign searches for things with that value
pollution %>% 
  filter(city != 'New York')

# ratio
storms$pressure / storms$wind
```

## tidyr

Two main functions: gather() and spread() 

```{r tidyr, eval=F}
# install.packages("tidyr")
library(tidyr)
?gather # gather to long
?spread # spread to wide
```

### `gather`

```{r gather, eval=F}
cases
gather(cases, "year", "n", 2:4)
gather(cases, "year", "n", -country)
cases %>% 
  gather("year", "n", -country) %>% 
  filter(year %in% c(2011,2013), 
         country %in% c('FR', 'US'))

## ! flips it all: 
cases %>%
  gather("year", "n", -country) %>% 
  filter(year %in% c(2011,2013), 
         !country %in% c('FR', 'US')) ##this selects all things NOT FR, US :-) 
```

### `spread`

```{r spread, eval=F}
pollution
spread(pollution, size, amount)
##data fram, colmn to use as keys, :amount' fills the cells 
```

Other functions to extract and combine columns...

### `separate`

```{r separate, eval=F}
storms
storms2 <- separate(storms, date, c("year", "month", "day"), sep = "-")

storms %>% 
  mutate(date_str= as.character(date))


storms3 <- separate(storms, date, c("year", "month", "day"), sep = c(4,6))
```

### `unite`

```{r unite, eval=F}
storms2
unite(storms2, "date", year, month, day, sep = "-")
```

**Recap: tidyr**:

- A package that reshapes the layout of data sets.

- Make observations from variables with `gather()` Make variables from observations with `spread()`

- Split and merge columns with `unite()` and `separate()`

From the [data-wrangling-cheatsheet.pdf](./refs/cheatsheets/data-wrangling-cheatsheet.pdf):

  ![](wk04_tidyr/img/data-wrangling-cheatsheet_tidyr.png)

### tidy CO<sub>2</sub> emissions
##Assignment 4

_**Task**. Convert the following table [CO<sub>2</sub> emissions per country since 1970](http://edgar.jrc.ec.europa.eu/overview.php?v=CO2ts1990-2014&sort=des9) from wide to long format and output the first few rows into your Rmarkdown. I recommend consulting `?gather` and you should have 3 columns in your output._

```{r read co2, eval=F}
library(readxl) # install.packages('readxl')

url = 'http://edgar.jrc.ec.europa.eu/news_docs/CO2_1970-2014_dataset_of_CO2_report_2015.xls'
xls = '../data/co2_europa.xls'

print(getwd())
if (!file.exists(xls)){
  download.file(url, xls, method='internal')
}
co2 = read_excel(xls, skip=12)
co2
```

_**Question**. Why use `skip=12` argument in `read_excel()`?_
The 'skip-12' command in the read_excel because the first 12 rows of the excel file are metadata-- titles, information about where the data came from etc. 

## dplyr

A package that helps transform tabular data

```{r dplyr, eval=F}
# install.packages("dplyr")
library(dplyr)
?select
?filter
?arrange
?mutate
?group_by
?summarise
```

See sections in the [data-wrangling-cheatsheet.pdf](./refs/cheatsheets/data-wrangling-cheatsheet.pdf):

- Subset Variables (Columns), eg `select()`
- Subset Observations (Rows), eg `filter()`
- Reshaping Data - Change the layout of a data set, eg `arrange()`
- Make New Variables, eg `mutate()`
- Group Data, eg `group_by()` and `summarise()`

### `select`

```{r select, eval=F}
storms
select(storms, storm, pressure)
storms %>% select(storm, pressure)
```

### `filter`

```{r filter, eval=F}
storms
filter(storms, wind >= 50)
storms %>% filter(wind >= 50)

storms %>%
  filter(wind >= 50) %>%
  select(storm, pressure)
```

### `mutate`

```{r mutate, eval=F}
storms %>%
  mutate(ratio = pressure / wind) %>%
  select(storm, ratio)
```

### `group_by`

```{r group_by, eval=F}
pollution
pollution %>% group_by(city)
```

### `summarise`

```{r summarise, eval=F}
# by city
pollution %>% 
  group_by(city) %>%
  summarise(
    mean = mean(amount), 
    sum = sum(amount), 
    n = n())

# by size
pollution %>% 
  group_by(size) %>%
  summarise(
    mean = mean(amount), 
    sum = sum(amount), 
    n = n())
```

note that `summarize` synonymously works

### `ungroup`

```{r ungroup, eval=F}
pollution %>% 
  group_by(size)

pollution %>% 
  group_by(size) %>%
  ungroup()
```

### multiple groups

```{r multiple groups, eval=F}
tb %>%
  group_by(country, year) %>%
  summarise(cases = sum(cases))
  summarise(cases = sum(cases))
```

**Recap: dplyr**:

- Extract columns with `select()` and rows with `filter()`

- Sort rows by column with `arrange()`

- Make new columns with `mutate()`

- Group rows by column with `group_by()` and `summarise()`

See sections in the [data-wrangling-cheatsheet.pdf](./refs/cheatsheets/data-wrangling-cheatsheet.pdf):

- Subset Variables (Columns), eg `select()`

- Subset Observations (Rows), eg `filter()`

- Reshaping Data - Change the layout of a data set, eg `arrange()`

- Make New Variables, eg `mutate()`

- Group Data, eg `group_by()` and `summarise()`


### summarize CO<sub>2</sub> emissions

_**Task**. Report the top 5 emitting countries (not World or EU28) for 2014 using your long format table. (You may need to convert your year column from factor to numeric, eg `mutate(year = as.numeric(as.character(year)))`. As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 

_**Task**. Summarize the total emissions by country  (not World or EU28) across years from your long format table and return the top 5 emitting countries. (As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 


## joining data

### `bind_cols`

```{r bind_cols, eval=F}
y = data.frame(
  x1 = c('A','B','C'), 
  x2 = c( 1 , 2 , 3), 
  stringsAsFactors=F)
z = data.frame(
  x1 = c('B','C','D'), 
  x2 = c( 2 , 3 , 4), 
  stringsAsFactors=F)
y
z
bind_cols(y, z)
```

### `bind_rows`

```{r bind_rows, eval=F}
y
z
bind_rows(y, z)
```

### `union`

```{r union, eval=F}
y
z
union(y, z)
```

### `intersect`

```{r intersect, eval=F}
y
z
intersect(y, z)
```

### `setdiff`

```{r setdiff, eval=F}
y
z
setdiff(y, z)
```

### `left_join`

```{r left_join, eval=F}
songs = data.frame(
  song = c('Across the Universe','Come Together', 'Hello, Goodbye', 'Peggy Sue'),
  name = c('John','John','Paul','Buddy'), 
  stringsAsFactors=F)
artists = data.frame(
  name = c('George','John','Paul','Ringo'),
  plays = c('sitar','guitar','bass','drums'), 
  stringsAsFactors=F)
left_join(songs, artists, by='name')
```

### `inner_join`

```{r inner_join, eval=F}
inner_join(songs, artists, by = "name")
```

### `semi_join`

```{r semi_join, eval=F}
semi_join(songs, artists, by = "name")
```

### `anti_join`

```{r anti_join, eval=F}
anti_join(songs, artists, by = "name")
```

### summarize per capita CO<sub>2</sub> emissions 

You'll join the [gapminder](https://github.com/jennybc/gapminder) datasets to get world population per country.

_**Task**. Report the top 5 emitting countries (not World or EU28) per capita using your long format table. (You may need to convert your year column from factor to numeric, eg `mutate(year = as.numeric(as.character(year)))`. As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 

_**Task**. Summarize the total emissions by country  (not World or EU28) per capita across years from your long format table and return the top 5 emitting countries. (As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 

```{r gapminder, eval=F}
library(gapminder) # install.packages('gapminder')
```
